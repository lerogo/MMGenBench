<!DOCTYPE html><html><head><meta charset=utf-8><meta name=description content=MMGenBench><meta name=keywords content=MMGenBench><meta name=viewport content="width=device-width, initial-scale=1"><title>MMGenBench</title><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css><link rel=stylesheet href=https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css><link rel=stylesheet href=./static/css/index.css><link rel=icon href="static/images/logo.svg?_t=1732247603"><link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel=stylesheet><script src=https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js></script><script defer src=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js></script><script type=module src=https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js></script></head><style>
  .section {
    margin-bottom: -30px;
    /* Adjust this value as needed to reduce the space */
  }

  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }

  .rainbow-text {
    background: linear-gradient(to right, #3498db, #6FB4DD);
    -webkit-background-clip: text;
    color: transparent;
    display: inline-block;
    font-weight: bold;
  }
</style><body><section class=hero><div class=hero-body><div class="container is-max-desktop"><div class="columns is-centered"><div class="column has-text-centered"><h1 class="title is-1 publication-title"><img id=logo width=10% style="pointer-events: none;" src="static/images/logo.svg?_t=1732247603"> <span class=rainbow-text>MMGenBench</span>: Evaluating the Limits of LMMs from the Text-to-Image Generation Perspective</h1><div class="is-size-5 publication-authors"><span class=author-block> <a href="https://scholar.google.com/citations?user=X0o0Ib8AAAAJ" target=_blank>Hailang Huang</a><sup><text style=color:#ffac33>1</text>,<text style=color:#ed4b82>2</text>*</sup>,</span>&nbsp; <span class=author-block> <a href=https://www.semanticscholar.org/author/Yong-Wang/1683878 target=_blank>Yong Wang</a><sup><text style=color:#ed4b82>2</text></text></sup>,</span>&nbsp; <span class=author-block> <a href="https://scholar.google.com.hk/citations?user=OPxHcAoAAAAJ" target=_blank>Zixuan Huang</a><sup><text style=color:#ffac33>1</text>,<text style=color:#ed4b82>2</text>*</sup>, </span>&nbsp; <span class=author-block> <a href="https://scholar.google.com/citations?user=zp8a-P8AAAAJ" target=_blank>Huaqiu Li</a><sup><text style=color:#ed4b82>2</text>,<text style=color:#6fbf73>3</text>*</sup>, </span>&nbsp; <span class=author-block> <a href="https://scholar.google.com/citations?user=eaZBubIAAAAJ" target=_blank>Tongwen Huang</a><sup><text style=color:#ed4b82>2</text></sup>, </span><br><span class=author-block> <a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ" target=_blank>Xiangxiang Chu</a><sup><text style=color:#ed4b82>2</text>&dagger;</sup>, </span>&nbsp; <span class=author-block> <a href="https://scholar.google.com/citations?user=bjFPXksAAAAJ" target=_blank>Richong Zhang</a><sup><text style=color:#ffac33>1</text>&Dagger;</sup></div><div class="is-size-5 publication-authors"><span class=author-block><sup><text style=color:#ffac33>1</text></sup>Beihang University,</span>&nbsp; <span class=author-block><sup><text style=color:#ed4b82>2</text></sup>Alibaba Group,</span>&nbsp; <span class=author-block><sup><text style=color:#6fbf73>3</text></sup>Tsinghua University</span></div><div class="is-size-6 publication-authors"><span class=author-block><sup>*</sup>Work done during an internship at Alibaba Group</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class=author-block><sup>&dagger;</sup>Project Leader</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class=author-block><sup>&Dagger;</sup>Corresponding Author</span></div><div class="column has-text-centered"><div class=publication-links><span class=link-block> <a href=https://arxiv.org/abs/2411.14062 target=_blank class="external-link button is-normal is-rounded is-dark"> <span class=icon> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span><span class=link-block> <a href=https://github.com/lerogo/MMGenBench target=_blank class="external-link button is-normal is-rounded is-dark"> <span class=icon> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span><span class=link-block> <a href=https://huggingface.co/datasets/lerogo/MMGenBench target=_blank class="external-link button is-normal is-rounded is-dark"><span class=icon>ðŸ¤—</span><span>Dataset</span> </a></span></div></div></div></div><section class=section><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-six-fifths"><div style="text-align: center;"><img id=teaser width=80% style="pointer-events: none;" src="static/images/dataset-case.svg?_t=1732247603"></div><br><h2 class="title is-3">Abstract</h2><div class="content has-text-justified"><p> Large Multimodal Models (LMMs) have demonstrated remarkable capabilities. While existing benchmarks for evaluating LMMs mainly focus on image comprehension, few works evaluate them from the image generation perspective. To address this issue, we propose a straightforward automated evaluation pipeline. Specifically, this pipeline requires LMMs to generate an image-prompt from a given input image. Subsequently, it employs text-to-image generative models to create a new image based on these generated prompts. Finally, we evaluate the performance of LMMs by comparing the original image with the generated one. Furthermore, we introduce <span class=rainbow-text>MMGenBench-Test</span>, a comprehensive benchmark developed to evaluate LMMs across 13 distinct image patterns, and <span class=rainbow-text>MMGenBench-Domain</span>, targeting the performance evaluation of LMMs within the generative image domain. A thorough evaluation involving over 50 popular LMMs demonstrates the effectiveness and reliability in both the pipeline and benchmark. Our observations indicate that numerous LMMs excelling in existing benchmarks fail to adequately complete the basic tasks, related to image understanding and description. This finding highlights the substantial potential for performance improvement in current LMMs and suggests avenues for future model optimization. Concurrently, our pipeline facilitates the efficient assessment of LMMs performance across diverse domains by using solely image inputs. </p></div></div></div><section class=section style=background-color:#efeff081 id=Highlight><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-six-fifths"><h2 class="title is-3">ðŸ”¥Highlight</h2><div class="content has-text-justified"><p style="font-size: 15px;"><ul><li>The proposed <span class=rainbow-text>MMGenBench</span> is the first automated pipeline, designed to evaluate the capabilities of LMMs in image understanding and description by solely utilizing images. This pipeline utilizes text-to-image models and image representation models for automated evaluation, thereby markedly minimizing human involvement and improving the efficiency and objectivity of the evaluation procedure.</li><li>We developed <span class=rainbow-text>MMGenBench-Test</span>, a comprehensive benchmark designed to evaluate LMMs across $13$ image patterns, and <span class=rainbow-text>MMGenBench-Domain</span>, which assesses the performance of LMMs in the generative image domain.</li><li>Our study includes a broad evaluation of over 50 popular LMMs, providing critical insights into their capabilities and limitations in basic image understanding and description tasks. </li></ul></p></div></div></div></div></section><br><section class=section id="Benchmark Overview"><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-six-fifths"><h2 class="title is-3"> <span class=rainbow-text>MMGenBench</span> Pipeline Overview</h2></div></div><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><div class="content has-text-justified"><p> The understanding and generation of images remain disparate fields, with the most formidable models in their respective domains adhering to distinct paradigms. For instance, GPT-4, which is grounded in the next token prediction paradigm, exhibits a formidable capacity for image comprehension. Similarly, Flux has achieved noteworthy success in text-to-image synthesis by leveraging diffusion models. This divergence underscores the complexity of achieving a unified approach to image processing and synthesis, as the state-of-the-art techniques continue to evolve along separate trajectories. Furthermore, LMMs are extensively employed to generate training data for generative models. It is noteworthy that LMMs excel in image-to-text tasks, while diffusion models are particularly effective in text-to-image tasks. A robust understanding of an image implies that LMMs can distill its essential information into text prompts, which can then be used by text-to-image models to reconstruct the scene to a certain extent. This process can be viewed as a form of "compression". <br> Hence, it is both reasonable and significant to evaluate the performance of LMMs using diffusion models. Our work aims to bridge this gap by providing a comprehensive evaluation pipeline. <br> We propose <span class=rainbow-text>MMGenBench-Test</span>, a comprehensive benchmark designed to evaluate LMMs across 13 distinct image patterns, and <span class=rainbow-text>MMGenBench-Domain</span>, which focuses on assessing LMMs performance within the generative image domain. To achieve this, we introduce a pipeline that initially allows LMMs to generate image-prompt from input images, then employs text-to-image generative models to create new images. Finally, we use an image representation model to obtain the embeddings of images and perform post-processing to assess the performance of LMMs in image understanding and description. The proposed pipeline based on text-to-image generative models and image representation models, consists of three components: image-prompt generation, new image generation, and quantitative metric computation. <centering><div style="text-align: center;"><img id=pipeline width=100% style="pointer-events: none;" src="static/images/pipeline.png?_t=1732247603"></div></p></div></b></div></div></div></section><section class=section id="Benchmark Construction"><div class="container is-max-desktop"><div class="columns is-centered has-text-centered"><div class="column is-six-fifths"><h2 class="title is-3"> <span class=rainbow-text>MMGenBench</span> Construction & Statistic</h2></div></div><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><div class="content has-text-justified"><p> To effectively measure the understanding and description capabilities of LMMs across various types of images, we constructed a high-quality test set <span class=rainbow-text>MMGenBench-Test</span> for 13 image patterns using the JourneyDB test set. We proposed a multi-stage method for extracting and annotating image patterns as illustrated in Figure. To ensure the results' accuracy, we manually double-checked the image patterns and performed the final annotations. In addition, we have also constructed a dataset in the "image generation" domain, termed <span class=rainbow-text>MMGenBench-Domain</span>, to evaluate the ability of LMMs in the understanding and describing "generated images" task. It is important to emphasize that our proposed pipeline can measure the ability of LMMs to understand and describe images in any domain. By utilizing images from a particular domain, we can easily assess the performance of LMMs specific to that domain. <centering><div style="text-align: center;"><img id=pipeline width=100% style="pointer-events: none;" src="static/images/MMGenBench-test.svg?_t=1732247603"></div></p><br><div style="display: flex;"><div style="flex: 0 0 65%;"><p> In the <span class=rainbow-text>MMGenBench-Test</span> dataset, we constructed a high-quality test set containing 1,284 images across 13 distinct image patterns. The distribution of images per pattern is shown in left Figure, which illustrates that each pattern contains a minimum of 114 images. Please note that an image may contain multiple patterns. For instance, the first image annotation in Previous Figure contains four image patterns: "Surreal", "Natural", "Artistic" and "Color". To construct <span class=rainbow-text>MMGenBench-Domain</span>, we randomly sampled 10,000 images from the JourneyDB validation set. By utilizing the proposed pipeline, we can evaluate the image understanding and descriptive performance of LMMs within this domain, obviating the need for additional data. </p></div><div style="flex: 0 0 34%;"><div style="text-align: center; margin-top: 0%;"><img id=pipeline width=100% style="pointer-events: none;" src="static/images/statistic.png?_t=1732247603"></div></div></div></div></b></div></div></div></section><section class=section id="Results on MMGenBench"><div class="columns is-centered has-text-centered"><div class="column is-six-fifths"><h2 class="title is-3"> Results on <span class=rainbow-text>MMGenBench</span> </h2></div></div><div class="container is-max-desktop"><div class="columns is-centered"><div class="column is-full-width"><div class="content has-text-justified"><p> The experimental evaluation of advanced LMMs on the <span class=rainbow-text>MMGenBench-Test</span> reveals that their SIM-Scores remain suboptimal, all falling below 0.600, with GPT-4o scoring 0.566 and the open-source InternVL2-76B achieving a slightly higher 0.599. Notably, there is no straightforward correlation between model size and performance, underscoring the critical roles of training data quality and training methodologies in enhancing image understanding and descriptive capabilities. LMMs that perform well on existing benchmarks, such as LLaVA-OV, underperform on <span class=rainbow-text>MMGenBench-Test</span>, highlighting the benchmark's unique challenges. <br> Further analysis on <span class=rainbow-text>MMGenBench-Domain</span> shows consistent SIM-Scores with <span class=rainbow-text>MMGenBench-Test</span> but introduces variations in FID-Scores due to the inclusion of a larger image set, leading to the recommendation of prioritizing SIM-Score as the primary evaluation metric. When dissecting image patterns, LMMs demonstrate robust performance on categories like "Artistic" and "Color" but struggle with "Contextual" and "Motion," indicating a proficiency in coarse-grained but not fine-grained image understanding. Investigations into model scalability reveal that increasing parameters within model series, such as Qwen2-VL and InternVL2, enhances performance, and improvements in training protocols and data further boost scores, as seen with Ovis1.6 outperforming Ovis1.5. However, three prevalent issues emerge: firstly, some LMMs fail to adhere strictly to instruction prompts, independent of their size; secondly, many LMMs cannot generate detailed image prompts due to training on datasets with short captions, limiting their descriptive depth; and thirdly, task-specific training leads to overfitting, as exemplified by xGen-MM's disproportionate emphasis on "safety" even when irrelevant. <br> These findings collectively suggest that while scaling and improved training can enhance LMM performance, ensuring strict adherence to instructions, fostering detailed descriptive abilities, and preventing overfitting are essential for developing more robust and versatile LMMs. <centering><div style="text-align: center;"><img id=teaser width=100% style="pointer-events: none;" src="static/images/result1.png?_t=1732247603"></div><br><div style="text-align: center;"><img id=teaser width=100% style="pointer-events: none;" src="static/images/result2.png?_t=1732247603"></div><br><div style="text-align: center;"><img id=teaser width=100% style="pointer-events: none;" src="static/images/result3.png?_t=1732247603"></div></p></div></b></div></div></div></section><section class=section id=BibTeX><div class="container is-max-desktop content"><h2 class=title>BibTeX</h2><pre><code>@misc{huang2024MMGenBench,
    title={MMGenBench: Evaluating the Limits of LMMs from the Text-to-Image Generation Perspective},
    author={Hailang Huang and Yong Wang and Zixuan Huang and Huaqiu Li and Tongwen Huang and Xiangxiang Chu and Richong Zhang},
    year={2024},
    eprint={2411.14062},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2411.14062}, 
}</code></pre></div></section><footer class=footer><div class="columns is-centered"><div class="column is-10"><div class=content><p style="text-align: center;"> This website is website adapted from <a href=https://nerfies.github.io/ >Nerfies</a>, licensed under a <a href=http://creativecommons.org/licenses/by-sa/4.0/ rel=license>Creative Commons Attribution-ShareAlike 4.0 International License</a>. </p></div></div></div></footer></body></html>